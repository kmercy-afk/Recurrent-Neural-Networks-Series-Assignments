{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output probabilities: [[0.49995477 0.50004523]\n",
      " [0.50000342 0.49999658]\n",
      " [0.50001503 0.49998497]\n",
      " [0.49996735 0.50003265]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ScratchSimpleRNNClassifier:\n",
    "    def __init__(self, n_features, n_nodes, n_output, activation=np.tanh):\n",
    "        \"\"\"\n",
    "        Initialize the RNN classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - n_features: Number of input features\n",
    "        - n_nodes: Number of nodes in the RNN layer\n",
    "        - n_output: Number of output classes\n",
    "        - activation: Activation function (default: tanh)\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_output = n_output\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.Wx = np.random.randn(n_features, n_nodes) * 0.01\n",
    "        self.Wh = np.random.randn(n_nodes, n_nodes) * 0.01\n",
    "        self.B = np.zeros(n_nodes)\n",
    "        self.W_out = np.random.randn(n_nodes, n_output) * 0.01\n",
    "        self.B_out = np.zeros(n_output)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the RNN.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data of shape (batch_size, n_sequences, n_features)\n",
    "\n",
    "        Returns:\n",
    "        - Output probabilities after the final fully connected layer\n",
    "        \"\"\"\n",
    "        batch_size, n_sequences, n_features = X.shape\n",
    "\n",
    "        # Initialize hidden state h0 to zeros\n",
    "        h_t = np.zeros((batch_size, self.n_nodes))\n",
    "        self.h_states = []  # To store all hidden states for each time step\n",
    "\n",
    "        for t in range(n_sequences):\n",
    "            x_t = X[:, t, :]  # Extract input at time t\n",
    "\n",
    "            # Compute activation and hidden state\n",
    "            a_t = np.dot(x_t, self.Wx) + np.dot(h_t, self.Wh) + self.B\n",
    "            h_t = self.activation(a_t)\n",
    "\n",
    "            # Store the hidden state\n",
    "            self.h_states.append(h_t)\n",
    "\n",
    "        # Use the last hidden state for classification (can be modified as needed)\n",
    "        h_last = self.h_states[-1]\n",
    "\n",
    "        # Fully connected layer to output\n",
    "        z_out = np.dot(h_last, self.W_out) + self.B_out\n",
    "\n",
    "        # Softmax for output probabilities\n",
    "        y_pred = self._softmax(z_out)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        \"\"\"\n",
    "        Compute the softmax function for the output layer.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input logits\n",
    "\n",
    "        Returns:\n",
    "        - Softmax probabilities\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability improvement\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Example usage:\n",
    "n_features = 3  # Number of input features\n",
    "n_nodes = 5     # Number of RNN nodes\n",
    "n_output = 2    # Number of output classes\n",
    "batch_size = 4\n",
    "n_sequences = 6\n",
    "\n",
    "# Initialize the model\n",
    "rnn = ScratchSimpleRNNClassifier(n_features, n_nodes, n_output)\n",
    "\n",
    "# Dummy input data (batch_size, n_sequences, n_features)\n",
    "X = np.random.randn(batch_size, n_sequences, n_features)\n",
    "\n",
    "# Forward propagation\n",
    "output = rnn.forward(X)\n",
    "print(\"Output probabilities:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given inputs\n",
    "x = np.array([[[1, 2], [2, 3], [3, 4]]]) / 100  # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]]) / 100  # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]]) / 100  # (n_nodes, n_nodes)\n",
    "b = np.array([1, 1, 1, 1])  # (n_nodes,)\n",
    "\n",
    "batch_size = x.shape[0]  # 1\n",
    "n_sequences = x.shape[1]  # 3\n",
    "n_features = x.shape[2]  # 2\n",
    "n_nodes = w_x.shape[1]  # 4\n",
    "\n",
    "# Initialize hidden state\n",
    "h = np.zeros((batch_size, n_nodes))  # (batch_size, n_nodes)\n",
    "\n",
    "# Forward propagation for the small array\n",
    "for t in range(n_sequences):\n",
    "    x_t = x[:, t, :]  # Input at time step t\n",
    "    a_t = np.dot(x_t, w_x) + np.dot(h, w_h) + b  # Linear transformation\n",
    "    h = np.tanh(a_t)  # Apply tanh activation\n",
    "\n",
    "# Final hidden state\n",
    "h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights Wx: [[-0.00906779 -0.03115919 -0.00489831  0.02224527  0.01096823]\n",
      " [ 0.00556174 -0.01704299  0.00776704 -0.0189186   0.02670235]\n",
      " [-0.00237759 -0.01454756 -0.00153554  0.00618662  0.00561791]]\n",
      "Updated weights Wh: [[-1.54403697e-03 -9.75842758e-03 -4.82742345e-03 -1.17631678e-03\n",
      "  -1.83959392e-02]\n",
      " [-2.38838167e-04  2.21249572e-03  9.32489828e-04 -9.32711316e-03\n",
      "   8.65597328e-03]\n",
      " [-2.85852162e-03 -6.65136276e-04  7.08011433e-03  2.52844326e-02\n",
      "  -6.26597809e-03]\n",
      " [-6.42058883e-04 -1.17437681e-03  3.55737406e-03 -7.27818626e-03\n",
      "  -2.98792290e-03]\n",
      " [-1.07230527e-02 -2.15891955e-03 -3.52296271e-04 -1.62774863e-02\n",
      "  -1.29308031e-05]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ScratchSimpleRNNClassifier:\n",
    "    def __init__(self, n_features, n_nodes, n_output, activation=np.tanh, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the RNN classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - n_features: Number of input features\n",
    "        - n_nodes: Number of nodes in the RNN layer\n",
    "        - n_output: Number of output classes\n",
    "        - activation: Activation function (default: tanh)\n",
    "        - learning_rate: Learning rate for weight updates\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_output = n_output\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.Wx = np.random.randn(n_features, n_nodes) * 0.01\n",
    "        self.Wh = np.random.randn(n_nodes, n_nodes) * 0.01\n",
    "        self.B = np.zeros(n_nodes)\n",
    "        self.W_out = np.random.randn(n_nodes, n_output) * 0.01\n",
    "        self.B_out = np.zeros(n_output)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the RNN.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data of shape (batch_size, n_sequences, n_features)\n",
    "\n",
    "        Returns:\n",
    "        - Output probabilities after the final fully connected layer\n",
    "        \"\"\"\n",
    "        batch_size, n_sequences, n_features = X.shape\n",
    "\n",
    "        # Initialize hidden state h0 to zeros\n",
    "        h_t = np.zeros((batch_size, self.n_nodes))\n",
    "        self.h_states = []  # To store all hidden states for each time step\n",
    "        self.a_states = []  # To store all activation states for backpropagation\n",
    "\n",
    "        for t in range(n_sequences):\n",
    "            x_t = X[:, t, :]  # Extract input at time t\n",
    "\n",
    "            # Compute activation and hidden state\n",
    "            a_t = np.dot(x_t, self.Wx) + np.dot(h_t, self.Wh) + self.B\n",
    "            h_t = self.activation(a_t)\n",
    "\n",
    "            # Store the hidden state and activation state\n",
    "            self.h_states.append(h_t)\n",
    "            self.a_states.append(a_t)\n",
    "\n",
    "        # Use the last hidden state for classification (can be modified as needed)\n",
    "        h_last = self.h_states[-1]\n",
    "\n",
    "        # Fully connected layer to output\n",
    "        z_out = np.dot(h_last, self.W_out) + self.B_out\n",
    "\n",
    "        # Softmax for output probabilities\n",
    "        y_pred = self._softmax(z_out)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to compute gradients and update weights.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data of shape (batch_size, n_sequences, n_features)\n",
    "        - y_true: True labels of shape (batch_size, n_output)\n",
    "        - y_pred: Predicted probabilities from forward propagation\n",
    "        \"\"\"\n",
    "        batch_size, n_sequences, n_features = X.shape\n",
    "\n",
    "        # Compute the output layer gradient\n",
    "        dL_dz_out = (y_pred - y_true) / batch_size  # Gradient of loss w.r.t. z_out\n",
    "\n",
    "        # Gradients for W_out and B_out\n",
    "        h_last = self.h_states[-1]\n",
    "        dL_dW_out = np.dot(h_last.T, dL_dz_out)\n",
    "        dL_dB_out = np.sum(dL_dz_out, axis=0)\n",
    "\n",
    "        # Backpropagate through time for RNN layer\n",
    "        dL_dh_t = np.dot(dL_dz_out, self.W_out.T)  # Initial gradient from output layer\n",
    "\n",
    "        # Initialize gradients for Wx, Wh, and B\n",
    "        dL_dWx = np.zeros_like(self.Wx)\n",
    "        dL_dWh = np.zeros_like(self.Wh)\n",
    "        dL_dB = np.zeros_like(self.B)\n",
    "\n",
    "        for t in reversed(range(n_sequences)):\n",
    "            x_t = X[:, t, :]\n",
    "            a_t = self.a_states[t]\n",
    "            h_prev = self.h_states[t - 1] if t > 0 else np.zeros_like(self.h_states[0])\n",
    "\n",
    "            # Gradient of tanh activation\n",
    "            dL_da_t = dL_dh_t * (1 - np.tanh(a_t) ** 2)\n",
    "\n",
    "            # Gradients for Wx, Wh, and B\n",
    "            dL_dWx += np.dot(x_t.T, dL_da_t)\n",
    "            dL_dWh += np.dot(h_prev.T, dL_da_t)\n",
    "            dL_dB += np.sum(dL_da_t, axis=0)\n",
    "\n",
    "            # Propagate the gradient to the previous time step\n",
    "            dL_dh_t = np.dot(dL_da_t, self.Wh.T)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.Wx -= self.learning_rate * dL_dWx\n",
    "        self.Wh -= self.learning_rate * dL_dWh\n",
    "        self.B -= self.learning_rate * dL_dB\n",
    "        self.W_out -= self.learning_rate * dL_dW_out\n",
    "        self.B_out -= self.learning_rate * dL_dB_out\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        \"\"\"\n",
    "        Compute the softmax function for the output layer.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input logits\n",
    "\n",
    "        Returns:\n",
    "        - Softmax probabilities\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability improvement\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Example usage:\n",
    "n_features = 3  # Number of input features\n",
    "n_nodes = 5     # Number of RNN nodes\n",
    "n_output = 2    # Number of output classes\n",
    "batch_size = 4\n",
    "n_sequences = 6\n",
    "\n",
    "# Initialize the model\n",
    "rnn = ScratchSimpleRNNClassifier(n_features, n_nodes, n_output)\n",
    "\n",
    "# Dummy input data (batch_size, n_sequences, n_features)\n",
    "X = np.random.randn(batch_size, n_sequences, n_features)\n",
    "\n",
    "# Dummy true labels (one-hot encoded)\n",
    "y_true = np.eye(n_output)[np.random.choice(n_output, batch_size)]\n",
    "\n",
    "# Forward propagation\n",
    "output = rnn.forward(X)\n",
    "\n",
    "# Backward propagation\n",
    "rnn.backward(X, y_true, output)\n",
    "\n",
    "print(\"Updated weights Wx:\", rnn.Wx)\n",
    "print(\"Updated weights Wh:\", rnn.Wh)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
